{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ed3265",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Importing Libraries](#chapter0)\n",
    "* [Importing Datasets](#chapter0.5)\n",
    "* [Data Cleaning](#chapter1)\n",
    "    * [Data Types](#section_1_1)\n",
    "    * [Feature Exploration](#section_1_2)\n",
    "    * [Handling Missing Values](#section_1_3)\n",
    "    * [Outlier Detection](#section_1_4)\n",
    "        * [Sub Section 1.1.1](#sub_section_1_1_1)\n",
    "* [Chapter 2](#chapter2)\n",
    "    * [Section 2.1](#section_2_1)\n",
    "        * [Sub Section 2.1.1](#sub_section_2_1_1)\n",
    "        * [Sub Section 2.1.2](#sub_section_2_1_2)\n",
    "    * [Section 2.2](#section_2_2)\n",
    "        * [Sub Section 2.2.1](#sub_section_2_2_1)\n",
    "        * [Sub Section 2.2.2](#sub_section_2_2_2)\n",
    "* [Chapter 3](#chapter3)\n",
    "    * [Section 3.1](#section_3_1)\n",
    "        * [Sub Section 3.1.1](#sub_section_3_1_1)\n",
    "        * [Sub Section 3.1.2](#sub_section_3_1_2)\n",
    "    * [Section 3.2](#section_3_2)\n",
    "        * [Sub Section 3.2.1](#sub_section_3_2_1)\n",
    "        * [Sub Section 3.2.2](#sub_section_3_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570bff96",
   "metadata": {},
   "source": [
    "## Importing Libraries <a class=\"anchor\" id=\"chapter0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "018f1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_columns = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da296d9f",
   "metadata": {},
   "source": [
    "## Importing Datasets <a class=\"anchor\" id=\"chapter0.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90ec8f4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-152119b46b8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#test_c=test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv('test.csv')\n",
    "#test_c=test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce751c",
   "metadata": {},
   "source": [
    "## Data Cleaning <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab1d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dimensions for Train Data:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b085f99fac34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data Dimensions for Train Data:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'   Number of Records:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'   Number of Features:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print('Data Dimensions for Train Data:')\n",
    "print('   Number of Records:', df.shape[0])\n",
    "print('   Number of Features:', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5ddf3",
   "metadata": {},
   "source": [
    "### Data Types <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd362de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-47309d48b5ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61cefc",
   "metadata": {},
   "source": [
    "### Feature Exploration <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7e7437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7d4a6eb8df58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature Names:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print('Feature Names:')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c26c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Info\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5b1249f11c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train set Info'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test set Info'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print('Train set Info')\n",
    "display(df.info())\n",
    "print(' ')\n",
    "print('test set Info')\n",
    "display(test.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e1cb6",
   "metadata": {},
   "source": [
    "### Handling Missing Values: No Missing Values <a class=\"anchor\" id=\"section_1_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0180ab",
   "metadata": {},
   "source": [
    "### Outlier Detection <a class=\"anchor\" id=\"section_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a89e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_function(dff, col_name):\n",
    "    \n",
    "    first_quartile = np.percentile(np.array(dff[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(dff[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(3*IQR)\n",
    "    lower_limit = first_quartile-(3*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in dff[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e62d04a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-332f3742fe6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# loop through all columns to see if there are any outliers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutlier_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There are {} outliers in {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlier_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# loop through all columns to see if there are any outliers\n",
    "for column in df.columns:\n",
    "    if outlier_function(df, column)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(df, column)[2], column))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed6be6",
   "metadata": {},
   "source": [
    "here we are going to take a closer look for the outlier elimination for the following columns:\n",
    "\n",
    "Horizontal_Distance_To_Hydrology\n",
    "Vertical_Distance_To_Hydrology\n",
    "Horizontal_Distance_To_Roadways\n",
    "Horizontal_Distance_To_Fire_Points\n",
    "\n",
    "We are not considering other columns for potential outlier elimination because their data range is already fixed between 0 and 255 (e.g. Hillsahde columns) or they seem like one-hot-encoded columns (e.g. Soil type and Wilderness areas).\n",
    "\n",
    "Recalling the data ranges of those 4 columns:\n",
    "\n",
    "Horizontal_Distance_To_Hydrology: 0, 1343\n",
    "Vertical_Distance_To_Hydrology: -146, 554\n",
    "Horizontal_Distance_To_Roadways: 0, 6890\n",
    "Horizaontal_Distance_To_Firepoints: 0, 6993\n",
    "    \n",
    "Considering the Horizaontal_Distance_To_Firepoints having the highest number of outliers and widest data range, I am going to remove outliers only from that column.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd236fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e544ffcec329>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# set the histogram, mean and median\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Cover_Type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Cover_Type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Cover_Type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"median\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the plot size\n",
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "\n",
    "# set the histogram, mean and median\n",
    "sns.distplot(df[\"Cover_Type\"], kde=False)\n",
    "plt.axvline(df[\"Cover_Type\"].mean(), linewidth=3, color='g', label=\"mean\", alpha=0.5)\n",
    "plt.axvline(df[\"Cover_Type\"].median(), linewidth=3, color='y', label=\"median\", alpha=0.5)\n",
    "\n",
    "# set title, legends and labels\n",
    "plt.xlabel(\"Cover_Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Trees/Labels/Cover_Types\", size=14)\n",
    "plt.legend([\"mean\", \"median\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0475c98",
   "metadata": {},
   "source": [
    "The training data set that we will use for the model building is evenly distributed across the seven cover types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640534ca",
   "metadata": {},
   "source": [
    "## Findings from Understand, Clean and Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b14d0",
   "metadata": {},
   "source": [
    "Training dataset has 15120 entries and 56 columns with headers appropriately named. Dataset is clean and well-formatted, meaning it had no NA values and every column has a numeric (float or integer) data type.\n",
    "\n",
    "4 columns had outliers, outliers of the Horizontal_Distance_To_Fire_Points is removed considering this column has a wider range and has the most number of outliers.\n",
    "\n",
    "Cover_Type is our label/target column. Wilderness_Area and Soil_Type columns might have binary values (0,1) if so, they are the one-hot-encoded columns of 4 wilderness areas and 40 soil types respectively. I am going to start exploratory data analysis by seeking answer to that suspicion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2126d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ece28",
   "metadata": {},
   "source": [
    "Checking if Wilderness_Area and Soil_Type columns have only binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1aa5da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-30a3b1001f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# list of columns of wilderness areas and soil types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mis_binary_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Wilderness\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Soil\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mis_binary_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# list of columns of wilderness areas and soil types\n",
    "is_binary_columns = [column for column in df.columns if (\"Wilderness\" in column) | (\"Soil\" in column)]\n",
    "pd.unique(df[is_binary_columns].values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416710b5",
   "metadata": {},
   "source": [
    "Yes, they only have binary values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24b862",
   "metadata": {},
   "source": [
    "## Can one Cover Type belong to multiple soil types and wilderness areas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e584492b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-eeac53ea313c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sum of all widerness area columns for train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"w_sum\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area1\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area2\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area3\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# sum of all widerness area columns for train set\n",
    "df[\"w_sum\"] = df[\"Wilderness_Area1\"] + df[\"Wilderness_Area2\"] + df[\"Wilderness_Area3\"] + df[\"Wilderness_Area4\"]\n",
    "print(df.w_sum.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fcb1cf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-16e4617a3ba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sum of all widerness area columns for test file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"w_sum\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area1\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area2\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area3\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wilderness_Area4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# sum of all widerness area columns for test file\n",
    "test[\"w_sum\"] = test[\"Wilderness_Area1\"] + test[\"Wilderness_Area2\"] + test[\"Wilderness_Area3\"] + test[\"Wilderness_Area4\"]\n",
    "print(test.w_sum.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "603f1ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3a54719da454>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create a list of soil_type columns for train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoil_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"Soil\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"soil_sum\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# sum of all soil type columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# create a list of soil_type columns for train set\n",
    "soil_columns = [c for c in df.columns if \"Soil\" in c]\n",
    "df[\"soil_sum\"] = 0\n",
    "\n",
    "# sum of all soil type columns\n",
    "for c in soil_columns:\n",
    "    df[\"soil_sum\"] += df[c]\n",
    "\n",
    "print(df.soil_sum.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e24631c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9a7da836b756>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"w_sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"soil_sum\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.drop(columns=[\"w_sum\", \"soil_sum\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d717ab4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8234ee576962>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create a list of soil_type columns for test file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoil_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"Soil\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"soil_sum\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# sum of all soil type columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# create a list of soil_type columns for test file\n",
    "soil_columns = [c for c in test.columns if \"Soil\" in c]\n",
    "test[\"soil_sum\"] = 0\n",
    "\n",
    "# sum of all soil type columns\n",
    "for c in soil_columns:\n",
    "    test[\"soil_sum\"] += test[c]\n",
    "\n",
    "print(test.soil_sum.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3fd2ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-74d60adc67d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"w_sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"soil_sum\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.drop(columns=[\"w_sum\", \"soil_sum\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41664787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b666bf274d0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842586d2",
   "metadata": {},
   "source": [
    "Wilderness_Area and Soil_Type1-40 having only binary values and only one soil_type or wilderness_area being equal to 1, shows that they are one-hot-encoded columns.\n",
    "\n",
    "One important thing about cover type are, they can only belong to one soil type or one wilderness area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925848a",
   "metadata": {},
   "source": [
    "Check if the Cover_Type shows non-uniform distribution among different Wilderness_Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one column as Wilderness_Area_Type and represent it as categorical data\n",
    "df_use['Wilderness_Area_Type'] = (df.iloc[:, 11:15] == 1).idxmax(1)\n",
    "\n",
    "#list of wilderness areas\n",
    "wilderness_areas = sorted(df['Wilderness_Area_Type'].value_counts().index.tolist())\n",
    "\n",
    "# distribution of the cover type in different wilderness areas\n",
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "# plot cover_type distribution for each wilderness area\n",
    "for area in wilderness_areas:\n",
    "    subset = df[df.Wilderness_Area_Type == area]\n",
    "    sns.kdeplot(subset[\"Cover_Type\"], label=area, linewidth=2)\n",
    "# set title, legends and labels\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Cover_Type\")\n",
    "plt.title(\"Density of Cover Types Among Different Wilderness Areas\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13453298",
   "metadata": {},
   "source": [
    "One of the other important finding: Wilderness area is an important feature to determine the cover type:\n",
    "\n",
    "Spruce/Fir, Lodgepole Pine and Krummholz (Cover_Type 1, 2, 7) mostly found in Rawah, Neota and Comanche Peak Wilderness Area(1,2 and 3).\n",
    "It is highly likely to find Ponderosa Pine (Cover_Type 3) in Cache la Poudre Wilderness Area (4) rather than other areas.\n",
    "Cottonwood/Willow (Cover_Type 4) seems to be found only in Cache la Poudre Wilderness Area (4).\n",
    "Aspen (Cover_Type 5) is equally likely to come from wilderness area Rawah and Comanche (1,3).\n",
    "Douglas-fir (Cover_Type 6) can be found in any of the wilderness areas.\n",
    "Note that, distribution of cover types extend more than the range because of the kernel density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814e029",
   "metadata": {},
   "source": [
    "Wilderness Area 3 is more diverse in soil type and cover type.\n",
    "Only soil types 1 through 20 is represented in Wilderss Area 4, thus cover types in that area grew with them.\n",
    "Cover type 7 seems to grow with soil types 25 through 40.\n",
    "Cover Type 5 and 6 can grow with most of the soil types.\n",
    "Cover Type 3 loves soil type 0 through 15.\n",
    "Cover Type 1 and 2 can grow with any soil type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000d2e6",
   "metadata": {},
   "source": [
    "### Visualize some collinear features with Cover_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35683521",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "# plot the first subplot\n",
    "plt.subplot(1,2,1)\n",
    "sns.scatterplot(x=\"Vertical_Distance_To_Hydrology\", y=\"Horizontal_Distance_To_Hydrology\", \n",
    "                hue=\"Cover_Type\", data=df, \n",
    "                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\n",
    "plt.title(\"Vertical_Distance_To_Hydrology VS Horizontal_Distance_To_Hydrology\", size=14)\n",
    "\n",
    "# plot the second subplot\n",
    "plt.subplot(1,2,2)\n",
    "sns.scatterplot(x=\"Elevation\", y=\"Slope\", \n",
    "                hue=\"Cover_Type\", data=df, \n",
    "                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\n",
    "plt.title(\"Elevation VS Slope\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003903c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,10))\n",
    "\n",
    "# plot the first subplot\n",
    "plt.subplot(1,2,1)\n",
    "sns.scatterplot(x=\"Hillshade_Noon\", y=\"Hillshade_3pm\", \n",
    "                hue=\"Cover_Type\", data=df, \n",
    "                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\n",
    "plt.title(\"Hillshade_Noon VS Hillshade_3pm\", size=14)\n",
    "\n",
    "# plot the second subplot\n",
    "plt.subplot(1,2,2)\n",
    "sns.scatterplot(x=\"Hillshade_9am\", y=\"Hillshade_3pm\", \n",
    "                hue=\"Cover_Type\", data=df, \n",
    "                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\n",
    "plt.title(\"Hillshade_9am VS Hillshade_3pm\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cef57",
   "metadata": {},
   "source": [
    "One of the features from the Hillshade_9am or Hillshade_3pm or Hillshade_Noon will be dropped when determining the training set. for that we can use be determined after looking at the Pearson Coeffiecients with the label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28172817",
   "metadata": {},
   "source": [
    "### Pearson Coefficients of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e601dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "# plot heatmap set the title\n",
    "colormap = plt.cm.RdBu\n",
    "sns.heatmap(df.corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=False, cmap=colormap, linecolor='white', annot=False)\n",
    "plt.title('Pearson Correlation of All Features', y=1.05, size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910a71a",
   "metadata": {},
   "source": [
    "Distributions:\n",
    "\n",
    "Hillshade_9am and Hillshade_Noon has bi-modal and left-skewed distributions.\n",
    "Horizontal_Distance_To_Firepoints, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Hydrology has bi-modal and right-skewed distributions.\n",
    "Elevation resembles a uniform distribution.\n",
    "Slope, Vertical_Distance_To_Hydrology, Hillshade_3pm shows a symmetric and bi-modal distribution.\n",
    "Some obvious relationships between the continuous features:\n",
    "\n",
    "Elevation and shows positive trend with following variables:\n",
    "Vertical_Distance_To_Hydrology\n",
    "Horizontal_Distance_To_Roadways\n",
    "Horizontal_Distance_To_Firepoints\n",
    "Horizontal_Distance_To_Hydrology\n",
    "As Aspect increases; Hillshade_Noon and Hillshade_3pm increases.\n",
    "Slope has negative trend with:\n",
    "Elevation\n",
    "Horizontal_Distance_To_Roadways\n",
    "Hillshade_9am, Hillshade_Noon and Hillshade_3pm\n",
    "Horizontal_Distance_To_Firepoints\n",
    "Horizontal_Distance_To_Hydrology has positive trend with:\n",
    "Horizontal_Distance_To_Firepoints\n",
    "Horizontal_Distance_To_Roadways\n",
    "Vertical_Distance_To_Hydrology\n",
    "Vertical_Distance_To_Hydrology - Slope and Vertical_Distance_To_Hydrology - Horizontal_Distance_To_Hydrology has obvious collinear relationship.\n",
    "As Horizontal_Distance_To_Roadways increases, Horizontal_Distance_To_Firepoints increases and Slope decreases.\n",
    "Hillshade_9am shows negative trend with Hillshade_3pm and Aspect, as Hillshade_9am increases Elevation increases.\n",
    "Hillshade_Noon has positive trend with:\n",
    "Elevation\n",
    "Aspect\n",
    "Horizontal_Distance_To_Roadways\n",
    "Hillshade_3pm\n",
    "Horizontal_Distance_To_Firepoints\n",
    "Hillshade_3pm shows perfect negative relationship with Hillshade_9am and perfect positive relationship with Hillshade_Noon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37420487",
   "metadata": {},
   "source": [
    "None of the features are significantly different effect on determining the label cover type.\n",
    "\n",
    "One interesting finding though, Soil Type 7 and 15 columns are blank in the heatmap, thus zero effect on determining the label Cover_Type.\n",
    "\n",
    "Approximately 5 (1 percent of all soil types) soil_type columns affects the cover type.\n",
    "\n",
    "Can we get a better picture if we use soil_type as one numeric column rather than seperate one-hot-encoded columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9522cb",
   "metadata": {},
   "source": [
    "## Pearson coefficients with numeric Soil_Type representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df\n",
    "test_use =test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d38718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot_encode(x):\n",
    "    ''' this function takes the start and end location of the one-hot-encoded column set and numeric column name to be created as arguments\n",
    "    1) transforms one-hot-encoded columns into one column consisting of column names with string data type\n",
    "    2) splits string column into the alphabetical and numerical characters\n",
    "    3) fetches numerical character and creates numeric column in the given dataframe\n",
    "    '''\n",
    "    x['String_Column'] = (x.iloc[:,x.columns.get_loc('Soil_Type1'):x.columns.get_loc('Soil_Type40')+1] == 1).idxmax(1)\n",
    "    x['Tuple_Column'] = x['String_Column'].apply(split_numbers_chars)\n",
    "    x['Soil_Type'] = x['Tuple_Column'].apply(lambda x: x[1]).astype('int64')\n",
    "    x.drop(columns=['String_Column','Tuple_Column'], inplace=True)\n",
    "    x['Soil_Type']=x['Soil_Type'].astype('category')\n",
    "    x = x.drop(x.iloc[:,x.columns.get_loc('Soil_Type1'):x.columns.get_loc('Soil_Type40')+1],axis = 1)\n",
    "    return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_one_hot_encode(df_use);\n",
    "reverse_one_hot_encode(test_use);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_variables = df_use.columns[1:11].tolist()\n",
    "\n",
    "# Create one column as Wilderness_Area_Type and represent it as categorical data in test file\n",
    "test_use['Wilderness_Area_Type'] = (test_use.iloc[:, 11:15] == 1).idxmax(1)\n",
    "\n",
    "#list of wilderness areas\n",
    "wilderness_areas = sorted(test_use['Wilderness_Area_Type'].value_counts().index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of numeric features and create a dataframe with them\n",
    "all_features_w_label = continuous_variables + wilderness_areas + [\"Soil_Type\"] + [\"Cover_Type\"]\n",
    "df_w_numeric_soil = df_use[all_features_w_label]\n",
    "\n",
    "# pearson coefficients with numeric soil type column\n",
    "\n",
    "correlations = pd.DataFrame(df_w_numeric_soil.corr())\n",
    "\n",
    "figsize=(40,20)\n",
    "\n",
    "# plot the heatmap\n",
    "colormap = plt.cm.RdBu\n",
    "sns.heatmap(correlations,linewidths=0.1, \n",
    "            square=False, cmap=colormap, linecolor='white', annot=False)\n",
    "plt.title('Pearson Correlation of Features with Numeric Soil_Type', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa961cd",
   "metadata": {},
   "source": [
    "\n",
    "## Findings From Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd54fee",
   "metadata": {},
   "source": [
    "\n",
    "Data set have balanced labels, resulting in equal number of cover types. This will be an advantage when it comes to apply classification ML models because, the model will have good chance to learn patterns of all labels, eliminating the probability of underfitting.\n",
    "\n",
    "Different wilderness areas consist of some specific cover type. Interestingly, Cottonwood/Willow, specifically likes to grow in wilderness area 4. While cover types 1, 2, 5 and 6 can grow in any soil type, other cover types grows more with specific soil types.\n",
    "\n",
    "Soil types are reverse-one-hot-encoded, meaning they are going to be included as numeric data in the training set and one-hot-encoded soil type columns will be excluded. With that way, there is a stronger correlation between soil type and Cover_Type. Numeric soil type column and other variables have pearson coefficients in the range of [-0.2, 0.1].\n",
    "\n",
    "Hillshade columns are collinear within each other and Hillshade_9am has the least importance in determining Cover_Type. Thus this column will be dropped in Part 3 for better interpretability of the future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "#### Hillshade noon and 3pm high collinearity, one will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47ce73",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcab8b5",
   "metadata": {},
   "source": [
    "### Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Euclidean Distance Function\n",
    "def split_numbers_chars(row):\n",
    "    '''This function fetches the numerical characters at the end of a string\n",
    "    and returns alphabetical character and numerical chaarcters respectively'''\n",
    "    head = row.rstrip('0123456789')\n",
    "    tail = row[len(head):]\n",
    "    return head, tail\n",
    "def reverse_one_hot_encode(x):\n",
    "    ''' this function takes the start and end location of the one-hot-encoded column set and numeric column name to be created as arguments\n",
    "    1) transforms one-hot-encoded columns into one column consisting of column names with string data type\n",
    "    2) splits string column into the alphabetical and numerical characters\n",
    "    3) fetches numerical character and creates numeric column in the given dataframe\n",
    "    '''\n",
    "    x['String_Column'] = (x.iloc[:,x.columns.get_loc('Soil_Type1'):x.columns.get_loc('Soil_Type40')+1] == 1).idxmax(1)\n",
    "    x['Tuple_Column'] = x['String_Column'].apply(split_numbers_chars)\n",
    "    x['Soil_Type'] = x['Tuple_Column'].apply(lambda x: x[1]).astype('int64')\n",
    "    x.drop(columns=['String_Column','Tuple_Column'], inplace=True)\n",
    "    x['Soil_Type']=x['Soil_Type'].astype('category')\n",
    "    x = x.drop(x.iloc[:,x.columns.get_loc('Soil_Type1'):x.columns.get_loc('Soil_Type40')+1],axis = 1)\n",
    "    return x;\n",
    "def Euc_distance(x):\n",
    "    a = np.power(x['Horizontal_Distance_To_Hydrology'],2)\n",
    "    b = np.power(x['Vertical_Distance_To_Hydrology'],2)\n",
    "    x['Euclidean_Distance_To_Hydrology'] = np.sqrt(a + b)\n",
    "    return x;\n",
    "## Euclidean Distance Function\n",
    "def Direction_Aspect(x):\n",
    "    aspect_labels = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    interval = np.linspace(11.25, 371.25, 17)\n",
    "    interval[0] = 0\n",
    "    ## Segment and sort into bins using pd cuts\n",
    "    x['Aspect_Direction'] = pd.cut(x['Aspect']+11.25, interval, right=True, labels=aspect_labels, ordered=False)\n",
    "    return x;\n",
    "### Obtaining avg distances for numerical variables(vertical)\n",
    "def mean_vertical(x):\n",
    "    a ='Vertical_Distance_To_Hydrology'\n",
    "    b ='Elevation'\n",
    "    x['avg_hydro_elev'] = (x[a]+x[b])/2\n",
    "    return x;\n",
    "### Obtaining avg distances for numerical variables (horizontal)\n",
    "def mean_horizontal(x):\n",
    "    a ='Horizontal_Distance_To_Hydrology'\n",
    "    b ='Horizontal_Distance_To_Roadways'\n",
    "    c='Horizontal_Distance_To_Fire_Points'\n",
    "    x['avg_hydro_fire'] = (x[a]+x[c])/2\n",
    "    x['avg_hydro_road'] = (x[a]+x[b])/2\n",
    "    x['avg_road_fire'] = (x[b]+x[c])/2\n",
    "    return x;\n",
    "def log_transform(x):\n",
    "    x['Horizontal_Distance_To_Hydrology_log']=[np.log(i) if i>=1 else i for i in np.log(x['Horizontal_Distance_To_Hydrology'])]\n",
    "    x['Horizontal_Distance_To_Roadways_log']=[np.log(i) if i>=1 else i for i in np.log(x['Horizontal_Distance_To_Roadways'])]\n",
    "    x['Horizontal_Distance_To_Fire_Points_log']=np.log(x['Horizontal_Distance_To_Fire_Points'])\n",
    "    #x['Vertical_Distance_To_Hydrology_log']=np.log(x['Vertical_Distance_To_Hydrology'])\n",
    "    #x['Elevation_log']=np.log(x['Elevation'])\n",
    "    x['Euclidean_Distance_To_Hydrology_log']=np.log(x['Euclidean_Distance_To_Hydrology'])\n",
    "    return x;\n",
    "\n",
    "def root_transform(x):\n",
    "    x['Horizontal_Distance_To_Hydrology_root']=np.sqrt(x['Horizontal_Distance_To_Hydrology'])\n",
    "    x['Horizontal_Distance_To_Roadways_root']=np.sqrt(x['Horizontal_Distance_To_Roadways'])\n",
    "    x['Horizontal_Distance_To_Fire_Points_root']=np.sqrt(x['Horizontal_Distance_To_Fire_Points'])\n",
    "    #x['Vertical_Distance_To_Hydrology_log']=np.sqrt(x['Vertical_Distance_To_Hydrology'])\n",
    "    #x['Elevation_root']=np.sqrt(x['Elevation'])\n",
    "    x['Euclidean_Distance_To_Hydrology_root']=np.sqrt(x['Euclidean_Distance_To_Hydrology'])\n",
    "\n",
    "    return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Euc_distance(df)\n",
    "df = Direction_Aspect(df)\n",
    "df = mean_vertical(df)\n",
    "df = mean_horizontal(df)\n",
    "#df = log_transform(df)\n",
    "df = root_transform(df)\n",
    "df = reverse_one_hot_encode(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08d0ba",
   "metadata": {},
   "source": [
    "## Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de483be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cats=['Aspect_Direction']\n",
    "target_encoding_cats=['Soil_Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cbbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    ('target', TargetEncoder())\n",
    "])\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('one-hot',OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "col_transformer = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline,target_encoding_cats),\n",
    "    ('cat_pipeline',cat_pipeline,ohe_cats)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid = {\"n_estimators\": [300, 500, 600, 800],\n",
    "       \"max_depth\": [None, 5, 10, 20, 30],\n",
    "       \"max_features\": [\"auto\", \"sqrt\"],\n",
    "       \"min_samples_split\": [2,4,6],\n",
    "       \"min_samples_leaf\": [1,2,4]}\n",
    "np.random.seed(42)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model= RandomizedSearchCV(estimator=clf,\n",
    "                          param_distributions=grid,\n",
    "                          n_iter=10,\n",
    "                          cv=5,\n",
    "                          verbose=2)\n",
    "\n",
    "# model = RandomForestRegressor(random_state=0)\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('col_trans', col_transformer),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "\n",
    "set_config(display='diagram')\n",
    "display(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Cover_Type']\n",
    "x = df.drop(\"Cover_Type\",axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train);\n",
    "#mean_absolute_percentage_error(y_test, y_preds), mean_absolute_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ad4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Euc_distance(test)\n",
    "test = Direction_Aspect(test)\n",
    "test = mean_vertical(test)\n",
    "test = mean_horizontal(test)\n",
    "test = root_transform(test)\n",
    "test = reverse_one_hot_encode(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a00d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds=pd.Series(model_pipeline.predict(test))\n",
    "y_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_pred= pd.Series(y_preds)\n",
    "test_id=pd.Series(test.Id)\n",
    "result = pd.DataFrame({'Id':test_id, 'Cover_Type':model_0_pred})\n",
    "result.set_index('Id', inplace = True)\n",
    "result.to_csv('prediction_model0.csv')#,sep=',', float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_## best estimators for random forest regressor\n",
    "#RandomForestClassifier(max_depth=30, min_samples_leaf=2, min_samples_split=4,\n",
    "#                       n_estimators=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c50a47",
   "metadata": {},
   "source": [
    "## Extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Score: 0.75238 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model After grid search\n",
    "model = ExtraTreesClassifier(max_features=None, min_samples_leaf=2, min_samples_split=6,\n",
    "                     n_estimators=900, random_state=2022)\n",
    "#No need to run grid search again, but if you do want to run it, erase the ''' on top and bottom\n",
    "# and comment the model variable on top with #\n",
    "'''\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid = {'n_estimators': [600,700,800,900,1000,1100,1200,1300,1400,1500],\n",
    "                       'min_samples_leaf': [1,2,3,4],\n",
    "                       'min_samples_split': [5,6,7,8, 9, 10, 11, 12],\n",
    "                       'max_features': ['auto', 'sqrt', 'log2', None]}\n",
    "np.random.seed(42)\n",
    "\n",
    "clf = ExtraTreesClassifier(random_state=2022)\n",
    "model= RandomizedSearchCV(estimator=clf,\n",
    "                          param_distributions=grid,\n",
    "                          n_iter=20,scoring = 'accuracy',\n",
    "                          cv=5,\n",
    "                          verbose=2,random_state=2022)\n",
    "\n",
    "'''\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('col_trans', col_transformer),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "\n",
    "set_config(display='diagram')\n",
    "display(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96089be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Euc_distance(test)\n",
    "test = Direction_Aspect(test)\n",
    "test = mean_vertical(test)\n",
    "test = mean_horizontal(test)\n",
    "test = root_transform(test)\n",
    "test = reverse_one_hot_encode(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c04d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds=pd.Series(model_pipeline.predict(test))\n",
    "y_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e59b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_pred= pd.Series(y_preds)\n",
    "test_id=pd.Series(test.Id)\n",
    "result = pd.DataFrame({'Id':test_id, 'Cover_Type':model_1_pred})\n",
    "result.set_index('Id', inplace = True)\n",
    "result.to_csv('prediction_model1.csv')#,sep=',', float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b0171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d75f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92f7579",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier + Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Train set\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming train set\n",
    "df = Euc_distance(df)\n",
    "df = Direction_Aspect(df)\n",
    "df = mean_vertical(df)\n",
    "df = mean_horizontal(df)\n",
    "#df = log_transform(df)\n",
    "df = root_transform(df)\n",
    "df = reverse_one_hot_encode(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining outlier function\n",
    "def outlier_function(dff, col_name):\n",
    "    \n",
    "    first_quartile = np.percentile(np.array(dff[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(dff[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(3*IQR)\n",
    "    lower_limit = first_quartile-(3*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in dff[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing outliers\n",
    "df = df[(df['Horizontal_Distance_To_Fire_Points'] > outlier_function(df, 'Horizontal_Distance_To_Fire_Points')[0]) &\n",
    "              (df['Horizontal_Distance_To_Fire_Points'] < outlier_function(df, 'Horizontal_Distance_To_Fire_Points')[1])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f11cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining x and y previous to split\n",
    "y = df['Cover_Type']\n",
    "x = df.drop(\"Cover_Type\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931cde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting train and test within train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining pipeline\n",
    "model = ExtraTreesClassifier(max_features=None, min_samples_leaf=2, min_samples_split=6,\n",
    "                     n_estimators=900, random_state=2022)\n",
    "#No need to run grid search again, but if you do want to run it, erase the ''' on top and bottom\n",
    "# and comment the model variable on top with #\n",
    "\n",
    "\n",
    "'''\n",
    "grid = {'n_estimators': [600,700,800,900,1000,1100,1200,1300,1400,1500],\n",
    "                       'min_samples_leaf': [1,2,3,4],\n",
    "                       'min_samples_split': [5,6,7,8, 9, 10, 11, 12],\n",
    "                       'max_features': ['auto', 'sqrt', 'log2', None]}\n",
    "np.random.seed(42)\n",
    "\n",
    "clf = ExtraTreesClassifier(random_state=2022)\n",
    "model= RandomizedSearchCV(estimator=clf,\n",
    "                          param_distributions=grid,\n",
    "                          n_iter=20,scoring = 'accuracy',\n",
    "                          cv=5,\n",
    "                          verbose=2,random_state=2022)\n",
    "'''\n",
    "\n",
    "# model = RandomForestRegressor(random_state=0)\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('col_trans', col_transformer),\n",
    "    ('model',model)\n",
    "])\n",
    "\n",
    "\n",
    "set_config(display='diagram')\n",
    "display(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training pipeline\n",
    "model_pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining test set\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8de881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying transformations to test set\n",
    "test = Euc_distance(test)\n",
    "test = Direction_Aspect(test)\n",
    "test = mean_vertical(test)\n",
    "test = mean_horizontal(test)\n",
    "test = root_transform(test)\n",
    "test = reverse_one_hot_encode(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f467bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions\n",
    "y_preds=pd.Series(model_pipeline.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exporting csv with predictions\n",
    "model_2_pred= pd.Series(y_preds)\n",
    "test_id=pd.Series(test.Id)\n",
    "result = pd.DataFrame({'Id':test_id, 'Cover_Type':model_2_pred})\n",
    "result.set_index('Id', inplace = True)\n",
    "result.to_csv('prediction_model2.csv')#,sep=',', float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182277e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e98098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8335ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print the most important features of a logreg classifier based on the coefficient values\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({'variable': feature_names, # Feature names\n",
    "                         'coefficient': clf.coef_[0] # Feature Coeficients\n",
    "                    }) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dummies, x_test_dummies, y_train_dummies, y_test_dummies = train_test_split(dummies, y, test_size=0.1,shuffle=True, random_state=2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mod = LogisticRegression(max_iter=20000,penalty='l2')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(ridge_mod, x_train_dummies, y_train_dummies, cv=5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(ridge_mod.fit(dummies,y), dummies.columns.get_level_values(0).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcaf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "ridge_mod_cv = LogisticRegressionCV(max_iter=18000,penalty='l2',Cs=alphas)\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(ridge_mod_cv, dummies, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc1722",
   "metadata": {},
   "source": [
    "## Chi_Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bfb33",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Apply ChiSquared to score the features based on the training data.\n",
    "chi2_selector = SelectKBest(score_func=chi2).fit(dummified_hr_df,y)\n",
    "\n",
    "\n",
    "# Sort the features according to the ChiSquared Scores\n",
    "indices = np.argsort(chi2_selector.scores_)[::-1]\n",
    "\n",
    "# Get the Features Names of the ordered Features\n",
    "chi2_features = []\n",
    "for i in range(len(dummified_hr_df.columns.get_level_values(0))):\n",
    "    chi2_features.append(dummified_hr_df.columns.get_level_values(0)[indices[i]])\n",
    "\n",
    "# Now plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.yticks(rotation='horizontal',fontsize=16)\n",
    "plt.barh(chi2_features, chi2_selector.scores_[indices[range(len(dummified_hr_df.columns.get_level_values(0)))]], color='r', align='center')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3a2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9fa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef120f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161f351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f50a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
